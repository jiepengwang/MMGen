<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Unified Multi-modal Image Generation and Understanding in One Go.">
  <meta name="keywords" content="Image diffusion, generation, understanding, multi-modal">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MMGen: Unified Multi-modal Image Generation and Understanding in One Go
</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://jiepengwang.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://jiepengwang.github.io/StructRe/">
            StrucRe
          </a>
          <a class="navbar-item" href="https://ruixu.me/html/ComboStoc/index.html">
            ComboStoc
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MMGen: Unified Multi-modal Image Generation and Understanding in One Go
</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jiepengwang.github.io/">Jiepeng Wang</a><sup>1,3,*</sup>,</span>
            <span class="author-block">
              <a href="https://derrickwang005.github.io/">Zhaoqing Wang</a><sup>2,3,*</sup>,</span>
            <span class="author-block">
              <a href="https://haopan.github.io/">Hao Pan</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://liuyuan-pal.github.io/">Yuan Liu</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://miracle-fmh.github.io/">Dongdong Yu</a><sup>3</sup>, 
            </span>
            <span class="author-block">
              <a href="https://changhu.wang/">Changhu Wang</a><sup>3,†</sup>,
            </span>
            <span class="author-block">
              <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html">Wenping Wang</a><sup>6</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Hong Kong,</span>
            <span class="author-block"><sup>2</sup>The University of Sydney,</span>
            <span class="author-block"><sup>3</sup>AIsphere,</span>
            <span class="author-block"><sup>4</sup>Tsinghua University,</span>
            <span class="author-block"><sup>5</sup>Hong Kong University of Science and Technology,</span>
            <span class="author-block"><sup>6</sup>Texas A&M University</span>
          </div>


          <div class="is-size-5 publication-authors" style="margin-top: 1em;">
            * equal contribution, † corresponding author
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2503.20644"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/jiepengwang/MMGen"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.png" alt="teaser image" style="width: 100%;">
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            A unified diffusion framework for multi-modal generation and understanding has the transformative potential to achieve seamless and controllable image diffusion and other cross-modal tasks. 
            In this paper, we introduce MMGen, a unified framework that integrates multiple generative tasks into a single diffusion model. This includes: (1) multi-modal category-conditioned generation, where multi-modal outputs are generated simultaneously through a single inference process, given category information; (2) multi-modal visual understanding, which accurately predicts depth, surface normals, and segmentation maps from RGB images; and (3) multi-modal conditioned generation, which produces corresponding RGB images based on specific modality conditions and other aligned modalities. Our approach develops a novel diffusion transformer that flexibly supports multi-modal output, along with a simple modality-decoupling strategy to unify various tasks. Extensive experiments and applications demonstrate the effectiveness and superiority of MMGen across diverse tasks and conditions, highlighting its potential for applications that require simultaneous generation and understanding.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


    <!-- Method. -->
    <div class="section">
      <div class="container is-max-desktop">
        <h2 class="title is-3" style="margin-top: 20px; align-self: flex-start;">Method overview</h2>
        <div class="content has-text-justified">
          <p>
            (1) MM Encoding: Given paired multi-modal images, we first use a shared pretrained VAE encoder to encode
            each modality into latent patch codes. 
            (2) MM Diffusion: Patch codes corresponding to the same image location are grouped to form the
            multi-modal patch input x0
            , which is blended with random noise to create the diffusion input xt. Conditioned on timestep y, category
            label t and task embedding et, the MM Diffusion model iteratively predicts the velocity, resulting in denoised multi-modal patches 
            x0_d. 
            (3)
            MM Decoding: Finally, these patches are reprojected to the original image locations for each modality and decoded back into image pixels
            using a shared pretrained VAE decoder         
           </p>
           <img src="./static/images/method.png" alt="teaser image" style="margin-left: 50px; width: 90%;">
        </div>
      </div>
    </div>
    <!--/ Method. -->

    <!-- Generation and Understanding. -->
    <div class="section">
      <div class="column is-centered">
          <h2 class="title is-3">Results: Generation and Understanding</h2>

          <h2 class="title is-5"> (a)  Multi-modal category-conditioned generation</h2>
          </p>Given the category information, 
          multi-modal images (i.e., rgb, depth, normal, semantic segmentation) are generated simultaneously 
          in a single diffusion process.
          </p>
          <img src="./static/images/cat-cond-gen.png" style="margin-left: 50px; width: 85%;">

          <h2 class="title is-5" style="margin-top: 50px;"> (b) Multi-modal conditioned generation</h2>
          </p>
            Given a
            fine-grained condition input (e.g., depth or normal, highlighted by yellow rectangles), 
            our model can accurately generate the corresponding
            rgb image and other aligned outputs in parallel. 
          </p>

          </p>
          (b.1) Depth-conditioned generation
          </p>
          <img src="./static/images/depth-cond-gen.png" style="margin-left: 50px; width: 85%;">
        
          </p>
          (b.2) Normal-conditioned generation
          </p>
          <img src="./static/images/normal-cond-gen.png" style="margin-left: 50px; width: 85%;">

          </p>
          (b.3) Segmentation-conditioned generation
          </p>
          <img src="./static/images/seg-cond-gen.png" style="margin-left: 50px; width: 85%;">


          <h2 class="title is-5" style="margin-top: 50px;"> (c) Multi-modal image visual understanding</h2>
          </p>
            Given a reference image (highlighted with yellow rectangles), our framework
            accurately estimates the associated depth, normal, and semantic segmentation results
          </p>


          <div class="columns is-centered">

            <!-- Visual Effects. -->
            <div class="column">
              <div class="content">
                <!-- <h2 class="title is-5">(c.1) ImageNet-1k</h2> -->
              </p>
              Visual understanding on ImageNet-1k validation set.
              </p>
              <img src="./static/images/img-understanding-imgnet1k.png" alt="teaser image" style="width: 100%;">
              </div>
            </div>
            <!--/ Visual Effects. -->
      
      
            <!-- Visual Effects. -->
            <div class="column">
              <div class="content">
                <!-- <h2 class="title is-5">(c.2) ScanNet</h2> -->
              </p>
               Zero-shot on ScanNet dataset. 
              </p>
              <img src="./static/images/img-understanding-scannet.png" alt="teaser image" style="width: 100%;">
              </div>
            </div>
            <!--/ Visual Effects. -->  
      
        </div>

        </div>
      </div>
      <!--/ Generation and Understanding. -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Applications</h2>
    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-5">(a) Image-to-image translation</h2>
        </p>
        Given a reference image, MMGen can interpret it into three visual modalities simultaneously. 
        Then, for each modality, we can feed into MMGen as conditions to generate a new image.
        </p>
        <img src="./static/images/Image2Image.png" alt="teaser image" style="width: 100%;">
        </div>
      </div>
      <!--/ Visual Effects. -->


      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-5">(b) 3D reconstruction</h2>
        </p>
         Starting from a depth
         map, MMGen generates three visual modalities simultaneously, especially segmentation, which can be used for 3D recon
         struction of foreground objects without the need to run an individual segmentation model. 
        </p>
        <img src="./static/images/3d_recon.png" alt="teaser image" style="width: 100%;">
        </div>
      </div>
      <!--/ Visual Effects. -->  

  </div>


  <div class="columns is-centered">
    <!-- Visual Effects. -->
    <div class="column">
      <div class="content">
        <h2 class="title is-5">(c) Adaptation to new modalities</h2>
      </p>
      To assess the feasibility
      of extending MMGen to new modalities, we conducted
      two experiments using a commonly used modality—Canny
      edge, including: (1) Finetune an existing
      modality to a new modality (seg →canny); (2) Add an additional
      modality to support generation of 5 modalities simultaneously.
      </p>
      <img src="./static/images/ft2new_modaility.png" alt="teaser image" style="width: 60%;">
      </div>
    </div>
    <!--/ Visual Effects. -->

</div>

</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{jiepeng2025mmgen,
  author    = {Wang, Jiepeng and Wang, Zhaoqing and Pan, Hao and Liu, Yuan and Yu, Dongdong and Wang, Changhu and Wang, Wenping},
  title     = {MMGen: Unified Multi-modal Image Generation and Understanding in One Go},
  journal   = {arXiv preprint arXiv:2503.20644},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="XXXX">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/jiepengwang" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is borrowed from <a rel="Nerfies"
            href="https://nerfies.github.io/">  Nerfies</a>
            and licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
